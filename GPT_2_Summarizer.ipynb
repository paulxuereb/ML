{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT-2-Summarizer.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paulxuereb/ML/blob/master/GPT_2_Summarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pzxl1vYX-1kk",
        "colab_type": "text"
      },
      "source": [
        "Setup:\n",
        "\n",
        "1) Make sure GPU is enabled, go to edit->notebook settings->Hardware Accelerator GPU\n",
        "\n",
        "2) Make a copy to your google drive, click on copy to drive in panel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW0abT07ZkhZ",
        "colab_type": "text"
      },
      "source": [
        "Note: Colab will reset after 12 hours make sure to save your model checkpoints to google drive around 10-11 hours mark or before, then go to runtime->reset all runtimes. Now copy your train model back into colab and start training again from the previous checkpoint."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLXW02eIYpcB",
        "colab_type": "text"
      },
      "source": [
        "clone and cd into repo, nshepperd's fork https://github.com/nshepperd/gpt-2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICYu3w9hIJkC",
        "colab_type": "code",
        "outputId": "2f2bc433-df88-4444-ad39-b927decea0ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "!git clone https://github.com/nshepperd/gpt-2.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'gpt-2'...\n",
            "remote: Enumerating objects: 1, done.\u001b[K\n",
            "remote: Counting objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 233 (delta 0), reused 1 (delta 0), pack-reused 232\u001b[K\n",
            "Receiving objects: 100% (233/233), 4.38 MiB | 14.61 MiB/s, done.\n",
            "Resolving deltas: 100% (123/123), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eEIs3ApZUVO",
        "colab_type": "code",
        "outputId": "3f552bda-1e81-4a8e-e907-4c6eca31e763",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd gpt-2"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gpt-2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtgNJ7OFs1Cw",
        "colab_type": "code",
        "outputId": "6f79f632-f108-4fd2-e612-27250544b899",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CONTRIBUTORS.md  Dockerfile.gpu     gpt-2-samples  requirements.txt  train.py\n",
            "DEVELOPERS.md\t download_model.py  LICENSE\t   src\n",
            "Dockerfile.cpu\t encode.py\t    README.md\t   train-horovod.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qtn1qZPgZLb0",
        "colab_type": "text"
      },
      "source": [
        "Install requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "434oOx0bZH6J",
        "colab_type": "code",
        "outputId": "b111560b-de92-4bf8-cb7d-a1945717a359",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        }
      },
      "source": [
        "!pip3 install -r requirements.txt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fire>=0.1.3 (from -r requirements.txt (line 1))\n",
            "  Downloading https://files.pythonhosted.org/packages/5a/b7/205702f348aab198baecd1d8344a90748cb68f53bdcd1cc30cbc08e47d3e/fire-0.1.3.tar.gz\n",
            "Collecting regex==2017.4.5 (from -r requirements.txt (line 2))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/62/c0c0d762ffd4ffaf39f372eb8561b8d491a11ace5a7884610424a8b40f95/regex-2017.04.05.tar.gz (601kB)\n",
            "\u001b[K     |████████████████████████████████| 604kB 9.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests==2.21.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (2.21.0)\n",
            "Collecting tqdm==4.31.1 (from -r requirements.txt (line 4))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/4b/c38b5144cf167c4f52288517436ccafefe9dc01b8d1c190e18a6b154cd4a/tqdm-4.31.1-py2.py3-none-any.whl (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 18.4MB/s \n",
            "\u001b[?25hCollecting toposort==1.5 (from -r requirements.txt (line 5))\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/8a/321cd8ea5f4a22a06e3ba30ef31ec33bea11a3443eeb1d89807640ee6ed4/toposort-1.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (1.24.2)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2019.3.9)\n",
            "Building wheels for collected packages: fire, regex\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/1a/4d/6b30377c3051e76559d1185c1dbbfff15aed31f87acdd14c22\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/07/38/3c16b529d50cb4e0cd3dbc7b75cece8a09c132692c74450b01\n",
            "Successfully built fire regex\n",
            "\u001b[31mERROR: spacy 2.0.18 has requirement regex==2018.01.10, but you'll have regex 2017.4.5 which is incompatible.\u001b[0m\n",
            "Installing collected packages: fire, regex, tqdm, toposort\n",
            "  Found existing installation: regex 2018.1.10\n",
            "    Uninstalling regex-2018.1.10:\n",
            "      Successfully uninstalled regex-2018.1.10\n",
            "  Found existing installation: tqdm 4.28.1\n",
            "    Uninstalling tqdm-4.28.1:\n",
            "      Successfully uninstalled tqdm-4.28.1\n",
            "Successfully installed fire-0.1.3 regex-2017.4.5 toposort-1.5 tqdm-4.31.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvUQhgK3PQ4L",
        "colab_type": "text"
      },
      "source": [
        "Mount drive to access google drive for saving and accessing checkpoints later. Have to log in to your google account"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNpf6R4ahYSN",
        "colab_type": "code",
        "outputId": "6aadd354-541c-4b55-fa7d-f6711a0e749e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1hrgeKFYsuE",
        "colab_type": "text"
      },
      "source": [
        "Download the model data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A498TySgHYyF",
        "colab": {}
      },
      "source": [
        "!python3 download_model.py 117M"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UDpEGjfO8Q2",
        "colab_type": "code",
        "outputId": "37c712cc-ca96-4309-aaf7-fb3177bc56f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "source": [
        "!python3 download_model.py 345M"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rFetching checkpoint:   0%|                                              | 0.00/77.0 [00:00<?, ?it/s]\rFetching checkpoint: 1.00kit [00:00, 579kit/s]                                                      \n",
            "\rFetching encoder.json:   0%|                                           | 0.00/1.04M [00:00<?, ?it/s]\rFetching encoder.json: 1.04Mit [00:00, 47.3Mit/s]                                                   \n",
            "Fetching hparams.json: 1.00kit [00:00, 640kit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 1.42Git [00:23, 60.6Mit/s]                                 \n",
            "Fetching model.ckpt.index: 11.0kit [00:00, 5.96Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 927kit [00:00, 42.1Mit/s]                                                 \n",
            "Fetching vocab.bpe: 457kit [00:00, 42.9Mit/s]                                                       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zq-YwRnNOBYO",
        "colab_type": "text"
      },
      "source": [
        "encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oJPQtdLbbeK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!export PYTHONIOENCODING=UTF-8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMhy2MtAiGad",
        "colab_type": "code",
        "outputId": "1ed310a3-6daf-4768-eabf-ff9ee906faf8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "!ls /content/gpt-2"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CONTRIBUTORS.md  download_model.py  models\t      train-horovod.py\n",
            "DEVELOPERS.md\t encode.py\t    README.md\t      train.py\n",
            "Dockerfile.cpu\t gpt-2-samples\t    requirements.txt\n",
            "Dockerfile.gpu\t LICENSE\t    src\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KzSbAvePgsI",
        "colab_type": "text"
      },
      "source": [
        "Fetch checkpoints if you have them saved in google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cA2Wk7yIPmS6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r /content/drive/My\\ Drive/checkpoint/ /content/gpt-2/ "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0p--9zwqQRTc",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Let's get our train on! In this case the file is A Tale of Two Cities (Charles Dickens) from Project Gutenberg. To change the dataset GPT-2 models will fine-tune on, change this URL to another .txt file, and change corresponding part of the next cell. Note that you can use small datasets if you want but you will have to be sure not to run the fine-tuning for too long or you will overfit badly. Roughly, expect interesting results within minutes to hours in the 1-10s of megabyte ballpark, and below this you may want to stop the run early as fine-tuning can be very fast."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfP7MFOxL6Ze",
        "colab_type": "code",
        "outputId": "01c15bd6-4894-4b67-9b61-6b5a74a1ef3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install toposort"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: toposort in /usr/local/lib/python3.6/dist-packages (1.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPfJ5b3CQXqr",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Start training, add --model_name '345M' to use 345 model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKcyFhTeLo4D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f48d646d-f584-4936-abff-0346b820b937"
      },
      "source": [
        "!ls '/content/drive/My Drive/cnndata' -ltr | wc -l"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15695\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhVvIaVHifaz",
        "colab_type": "code",
        "outputId": "cf71dfe8-d2b5-4cb4-99dd-fba6eb6de836",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        }
      },
      "source": [
        "!PYTHONPATH=src ./train.py --dataset '/content/drive/My Drive/cnndata' --model_name '345M'"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-05-09 10:43:44.782491: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-05-09 10:43:44.786828: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x191eaa0 executing computations on platform Host. Devices:\n",
            "2019-05-09 10:43:44.786919: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:51: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:53: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.random.categorical instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "Loading checkpoint models/345M/model.ckpt\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "Loading dataset...\n",
            " 29% 2537/8676 [09:55<23:09,  4.42it/s]Traceback (most recent call last):\n",
            "  File \"./train.py\", line 266, in <module>\n",
            "    main()\n",
            "  File \"./train.py\", line 146, in main\n",
            "    chunks = load_dataset(enc, args.dataset, args.combine)\n",
            "  File \"/content/gpt-2/src/load_dataset.py\", line 33, in load_dataset\n",
            "    raw_text += fp.read()\n",
            "  File \"/usr/lib/python3.6/codecs.py\", line 318, in decode\n",
            "    def decode(self, input, final=False):\n",
            "KeyboardInterrupt\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vS1RJJDFOPnb",
        "colab_type": "text"
      },
      "source": [
        "Save our checkpoints to start training again later"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JretqG1zOXdi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r /content/gpt-2/checkpoint/ /content/drive/My\\ Drive/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6D-i7vERWbNS",
        "colab_type": "text"
      },
      "source": [
        "Load your trained model for use in sampling below (117M or 345M)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeETvWvrbKga",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r /content/gpt-2/checkpoint/run1/* /content/gpt-2/models/117M/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "np0r6qfXBeUX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r /content/gpt-2/checkpoint/run1/* /content/gpt-2/models/345M/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmnSrXqtfRbq",
        "colab_type": "text"
      },
      "source": [
        "We test GPT-2’s ability to perform summarization on the\n",
        "CNN and Daily Mail dataset (Nallapati et al., 2016). To induce summarization behavior we add the text TL;DR: after\n",
        "the article and generate 100 tokens with Top-k random sampling (Fan et al., 2018) with k = 2 which reduces repetition\n",
        "and encourages more abstractive summaries than greedy decoding. We use the first 3 generated sentences in these 100\n",
        "tokens as the summary. While qualitatively the generations\n",
        "resemble summaries, as shown in Table 14, they often focus\n",
        "on recent content from the article or confuse specific details\n",
        "such as how many cars were involved in a crash or whether\n",
        "a logo was on a hat or shirt. On the commonly reported\n",
        "ROUGE 1,2,L metrics the generated summaries only begin\n",
        "to approach the performance of classic neural baselines and\n",
        "just barely outperforms selecting 3 random sentences from\n",
        "the article. GPT-2’s performance drops by 6.4 points on\n",
        "the aggregate metric when the task hint is removed which\n",
        "demonstrates the ability to invoke task specific behavior in\n",
        "a language model with natural language.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utJj-iY4gHwE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python3 src/interactive_conditional_samples.py --top_k 2 --model_name \"345M\" --length 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeDhY97XMDXn",
        "colab_type": "text"
      },
      "source": [
        "To check flag descriptions, use:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBaj2L_KMAgb",
        "colab_type": "code",
        "outputId": "fd0c56cb-1578-4a24-9457-63c9dd154797",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "!python3 src/interactive_conditional_samples.py -- --help"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Type:        function\n",
            "String form: <function interact_model at 0x7f97d2e1ad08>\n",
            "File:        /content/gpt-2/src/interactive_conditional_samples.py\n",
            "Line:        11\n",
            "Docstring:   Interactively run the model\n",
            ":model_name=117M : String, which model to use\n",
            ":seed=None : Integer seed for random number generators, fix seed to reproduce\n",
            " results\n",
            ":nsamples=1 : Number of samples to return total\n",
            ":batch_size=1 : Number of batches (only affects speed/memory).  Must divide nsamples.\n",
            ":length=None : Number of tokens in generated text, if None (default), is\n",
            " determined by model hyperparameters\n",
            ":temperature=1 : Float value controlling randomness in boltzmann\n",
            " distribution. Lower temperature results in less random completions. As the\n",
            " temperature approaches zero, the model will become deterministic and\n",
            " repetitive. Higher temperature results in more random completions.\n",
            ":top_k=0 : Integer value controlling diversity. 1 means only 1 word is\n",
            " considered for each step (token), resulting in deterministic completions,\n",
            " while 40 means 40 words are considered at each step. 0 (default) is a\n",
            " special setting meaning no restrictions. 40 generally is a good value.\n",
            "\n",
            "Usage:       interactive_conditional_samples.py [MODEL_NAME] [SEED] [NSAMPLES] [BATCH_SIZE] [LENGTH] [TEMPERATURE] [TOP_K]\n",
            "             interactive_conditional_samples.py [--model-name MODEL_NAME] [--seed SEED] [--nsamples NSAMPLES] [--batch-size BATCH_SIZE] [--length LENGTH] [--temperature TEMPERATURE] [--top-k TOP_K]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}