{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seq2SeqAllenNLP-ChitChat.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paulxuereb/ML/blob/master/Seq2SeqAllenNLP_ChitChat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "fRhwNb-LG7jC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "http://www.realworldnlpbook.com/blog/building-seq2seq-machine-translation-models-using-allennlp.html"
      ]
    },
    {
      "metadata": {
        "id": "PDiQMwzoG4qZ",
        "colab_type": "code",
        "outputId": "532a6ad9-6130-439a-c725-225283331c82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1900
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install AllenNLP"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: AllenNLP in /usr/local/lib/python3.6/dist-packages (0.8.3)\n",
            "Requirement already satisfied: overrides in /usr/local/lib/python3.6/dist-packages (from AllenNLP) (1.9)\n",
            "Requirement already satisfied: conllu==0.11 in /usr/local/lib/python3.6/dist-packages (from AllenNLP) (0.11)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.6/dist-packages (from AllenNLP) (5.5.1)\n",
            "Requirement already satisfied: flaky in /usr/local/lib/python3.6/dist-packages (from AllenNLP) (3.5.3)\n",
            "Requirement already satisfied: msgpack<0.6.0,>=0.5.6 in /usr/local/lib/python3.6/dist-packages (from AllenNLP) (0.5.6)\n",
            "Requirement already satisfied: parsimonious>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from AllenNLP) (0.8.1)\n",
            "Requirement already satisfied: numpydoc>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from AllenNLP) (0.9.1)\n",
            "Requirement already satisfied: moto>=1.3.4 in /usr/local/lib/python3.6/dist-packages (from AllenNLP) (1.3.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from AllenNLP) (2.8.0)\n",
            "Requirement already satisfied: tensorboardX>=1.2 in /usr/local/lib/python3.6/dist-packages (from AllenNLP) (1.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from AllenNLP) (0.20.3)\n",
            "Requirement already satisfied: sqlparse>=0.2.4 in /usr/local/lib/python3.6/dist-packages (from AllenNLP) (0.3.0)\n",
            "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.6/dist-packages (from AllenNLP) (4.28.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from AllenNLP) (1.2.1)\n",
            "Requirement already satisfied: flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from AllenNLP) (1.0.2)\n",
            "Requirement already satisfied: awscli>=1.11.91 in /usr/local/lib/python3.6/dist-packages (from AllenNLP) (1.16.147)\n",
            "Requirement already satisfied: responses>=0.7 in /usr/local/lib/python3.6/dist-packages (from AllenNLP) (0.10.6)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from AllenNLP) (3.2.5)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from AllenNLP) (0.5.3)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.6/dist-packages (from AllenNLP) (2.21.0)\n",
            "Requirement already satisfied: spacy<2.2,>=2.0 in /usr/local/lib/python3.6/dist-packages (from AllenNLP) (2.0.18)\n",
            "Requirement already satisfied: word2number>=1.1 in /usr/local/lib/python3.6/dist-packages (from AllenNLP) (1.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.6/dist-packages (from AllenNLP) (2018.9)\n",
            "Requirement already satisfied: jsonnet>=0.10.0; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from AllenNLP) (0.12.1)\n",
            "Requirement already satisfied: gevent>=1.3.6 in /usr/local/lib/python3.6/dist-packages (from AllenNLP) (1.4.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from AllenNLP) (3.6.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from AllenNLP) (1.16.3)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from AllenNLP) (3.0.3)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.6/dist-packages (from AllenNLP) (1.0.23)\n",
            "Requirement already satisfied: pytorch-pretrained-bert>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from AllenNLP) (0.6.2)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from AllenNLP) (1.0.1.post2)\n",
            "Requirement already satisfied: flask-cors>=3.0.7 in /usr/local/lib/python3.6/dist-packages (from AllenNLP) (3.0.7)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from AllenNLP) (1.9.134)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->AllenNLP) (0.1.7)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from parsimonious>=0.8.0->AllenNLP) (1.12.0)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->AllenNLP) (2.10.1)\n",
            "Requirement already satisfied: sphinx>=1.6.5 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->AllenNLP) (1.8.5)\n",
            "Requirement already satisfied: botocore>=1.12.86 in /usr/local/lib/python3.6/dist-packages (from moto>=1.3.4->AllenNLP) (1.12.134)\n",
            "Requirement already satisfied: python-jose<4.0.0 in /usr/local/lib/python3.6/dist-packages (from moto>=1.3.4->AllenNLP) (3.0.1)\n",
            "Requirement already satisfied: boto>=2.36.0 in /usr/local/lib/python3.6/dist-packages (from moto>=1.3.4->AllenNLP) (2.49.0)\n",
            "Requirement already satisfied: mock in /usr/local/lib/python3.6/dist-packages (from moto>=1.3.4->AllenNLP) (2.0.0)\n",
            "Requirement already satisfied: aws-xray-sdk!=0.96,>=0.93 in /usr/local/lib/python3.6/dist-packages (from moto>=1.3.4->AllenNLP) (2.4.2)\n",
            "Requirement already satisfied: docker>=2.5.1 in /usr/local/lib/python3.6/dist-packages (from moto>=1.3.4->AllenNLP) (3.7.2)\n",
            "Requirement already satisfied: werkzeug in /usr/local/lib/python3.6/dist-packages (from moto>=1.3.4->AllenNLP) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from moto>=1.3.4->AllenNLP) (2.5.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from moto>=1.3.4->AllenNLP) (3.13)\n",
            "Requirement already satisfied: jsondiff==1.1.2 in /usr/local/lib/python3.6/dist-packages (from moto>=1.3.4->AllenNLP) (1.1.2)\n",
            "Requirement already satisfied: cryptography>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from moto>=1.3.4->AllenNLP) (2.6.1)\n",
            "Requirement already satisfied: xmltodict in /usr/local/lib/python3.6/dist-packages (from moto>=1.3.4->AllenNLP) (0.12.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from moto>=1.3.4->AllenNLP) (2.8)\n",
            "Requirement already satisfied: cfn-lint in /usr/local/lib/python3.6/dist-packages (from moto>=1.3.4->AllenNLP) (0.19.1)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->AllenNLP) (3.7.1)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->AllenNLP) (7.0)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->AllenNLP) (1.1.0)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.91->AllenNLP) (0.2.0)\n",
            "Requirement already satisfied: colorama<=0.3.9,>=0.2.5 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.91->AllenNLP) (0.3.9)\n",
            "Requirement already satisfied: rsa<=3.5.0,>=3.1.2 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.91->AllenNLP) (3.4.2)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.91->AllenNLP) (0.14)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->AllenNLP) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->AllenNLP) (1.24.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->AllenNLP) (2019.3.9)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0->AllenNLP) (1.0.2)\n",
            "Requirement already satisfied: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0->AllenNLP) (0.2.9)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0->AllenNLP) (2.0.1)\n",
            "Requirement already satisfied: ujson>=1.35 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0->AllenNLP) (1.35)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0->AllenNLP) (0.9.6)\n",
            "Requirement already satisfied: regex==2018.01.10 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0->AllenNLP) (2018.1.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0->AllenNLP) (2.0.2)\n",
            "Requirement already satisfied: thinc<6.13.0,>=6.12.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0->AllenNLP) (6.12.1)\n",
            "Requirement already satisfied: greenlet>=0.4.14; platform_python_implementation == \"CPython\" in /usr/local/lib/python3.6/dist-packages (from gevent>=1.3.6->AllenNLP) (0.4.15)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->AllenNLP) (1.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->AllenNLP) (40.9.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->AllenNLP) (0.7.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->AllenNLP) (19.1.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->AllenNLP) (7.0.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->AllenNLP) (1.8.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->AllenNLP) (1.0.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->AllenNLP) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->AllenNLP) (2.4.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->AllenNLP) (0.9.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.3->numpydoc>=0.8.0->AllenNLP) (1.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->AllenNLP) (19.0)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->AllenNLP) (1.2.1)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->AllenNLP) (1.1.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->AllenNLP) (2.1.3)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->AllenNLP) (2.6.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->AllenNLP) (0.7.12)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->AllenNLP) (1.1.0)\n",
            "Requirement already satisfied: ecdsa<1.0 in /usr/local/lib/python3.6/dist-packages (from python-jose<4.0.0->moto>=1.3.4->AllenNLP) (0.13.2)\n",
            "Requirement already satisfied: future<1.0 in /usr/local/lib/python3.6/dist-packages (from python-jose<4.0.0->moto>=1.3.4->AllenNLP) (0.16.0)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python3.6/dist-packages (from mock->moto>=1.3.4->AllenNLP) (5.1.3)\n",
            "Requirement already satisfied: jsonpickle in /usr/local/lib/python3.6/dist-packages (from aws-xray-sdk!=0.96,>=0.93->moto>=1.3.4->AllenNLP) (1.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from aws-xray-sdk!=0.96,>=0.93->moto>=1.3.4->AllenNLP) (1.10.11)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from docker>=2.5.1->moto>=1.3.4->AllenNLP) (0.4.0)\n",
            "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.6/dist-packages (from docker>=2.5.1->moto>=1.3.4->AllenNLP) (0.56.0)\n",
            "Requirement already satisfied: asn1crypto>=0.21.0 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.3.0->moto>=1.3.4->AllenNLP) (0.24.0)\n",
            "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.3.0->moto>=1.3.4->AllenNLP) (1.12.3)\n",
            "Requirement already satisfied: aws-sam-translator>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from cfn-lint->moto>=1.3.4->AllenNLP) (1.11.0)\n",
            "Requirement already satisfied: jsonpatch in /usr/local/lib/python3.6/dist-packages (from cfn-lint->moto>=1.3.4->AllenNLP) (1.23)\n",
            "Requirement already satisfied: jsonschema~=2.6 in /usr/local/lib/python3.6/dist-packages (from cfn-lint->moto>=1.3.4->AllenNLP) (2.6.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<=3.5.0,>=3.1.2->awscli>=1.11.91->AllenNLP) (0.4.5)\n",
            "Requirement already satisfied: cytoolz<0.10,>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy<2.2,>=2.0->AllenNLP) (0.9.0.1)\n",
            "Requirement already satisfied: msgpack-numpy<0.4.4 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy<2.2,>=2.0->AllenNLP) (0.4.3.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.3.0->moto>=1.3.4->AllenNLP) (2.19)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.6/dist-packages (from jsonpatch->cfn-lint->moto>=1.3.4->AllenNLP) (2.0)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.10,>=0.9.0->thinc<6.13.0,>=6.12.1->spacy<2.2,>=2.0->AllenNLP) (0.9.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xuF4XVB4HEyJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "    \n",
        "import itertools\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from allennlp.data.dataset_readers.seq2seq import Seq2SeqDatasetReader\n",
        "from allennlp.data.iterators import BucketIterator\n",
        "from allennlp.data.token_indexers import SingleIdTokenIndexer\n",
        "from allennlp.data.tokenizers.character_tokenizer import CharacterTokenizer\n",
        "from allennlp.data.tokenizers.word_tokenizer import WordTokenizer\n",
        "from allennlp.data.vocabulary import Vocabulary\n",
        "from allennlp.nn.activations import Activation\n",
        "from allennlp.models.encoder_decoders.simple_seq2seq import SimpleSeq2Seq\n",
        "from allennlp.modules.attention import LinearAttention, BilinearAttention, DotProductAttention\n",
        "from allennlp.modules.seq2seq_encoders import PytorchSeq2SeqWrapper, StackedSelfAttentionEncoder\n",
        "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
        "from allennlp.modules.token_embedders import Embedding\n",
        "from allennlp.predictors import SimpleSeq2SeqPredictor\n",
        "from allennlp.training.trainer import Trainer\n",
        "from allennlp.modules.token_embedders.bert_token_embedder import PretrainedBertEmbedder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kcmDhjkTJ1DH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Setup data files"
      ]
    },
    {
      "metadata": {
        "id": "CHYbBnnRHhuO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3RpzW1tYHq_Y",
        "colab_type": "code",
        "outputId": "50daed83-ffcb-474a-f5ef-092057c48430",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "%cd data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "i07O2YhxJbS2",
        "colab_type": "code",
        "outputId": "38eed2c2-187e-462a-db5f-83cf698558cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        }
      },
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/mhagiwara/realworldnlp/master/examples/mt/create_bitext.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-04-28 09:34:34--  https://raw.githubusercontent.com/mhagiwara/realworldnlp/master/examples/mt/create_bitext.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4870 (4.8K) [text/plain]\n",
            "Saving to: ‘create_bitext.py’\n",
            "\n",
            "\rcreate_bitext.py      0%[                    ]       0  --.-KB/s               \rcreate_bitext.py    100%[===================>]   4.76K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-04-28 09:34:34 (97.4 MB/s) - ‘create_bitext.py’ saved [4870/4870]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "guj13h_970VP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Grab the Microsoft chat files"
      ]
    },
    {
      "metadata": {
        "id": "RRoGGRQX6T46",
        "colab_type": "code",
        "outputId": "35055e97-6713-4d58-85f1-f6a563db0a23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        }
      },
      "cell_type": "code",
      "source": [
        "!wget https://qnamakerstore.blob.core.windows.net/qnamakerdata/editorial/qna_chitchat_the_professional.tsv"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-04-28 09:34:48--  https://qnamakerstore.blob.core.windows.net/qnamakerdata/editorial/qna_chitchat_the_professional.tsv\n",
            "Resolving qnamakerstore.blob.core.windows.net (qnamakerstore.blob.core.windows.net)... 13.88.144.240\n",
            "Connecting to qnamakerstore.blob.core.windows.net (qnamakerstore.blob.core.windows.net)|13.88.144.240|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 62473 (61K) [text/tab-separated-values]\n",
            "Saving to: ‘qna_chitchat_the_professional.tsv’\n",
            "\n",
            "\r          qna_chitc   0%[                    ]       0  --.-KB/s               \rqna_chitchat_the_pr 100%[===================>]  61.01K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2019-04-28 09:34:49 (584 KB/s) - ‘qna_chitchat_the_professional.tsv’ saved [62473/62473]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "y52DdvOc6X0Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "outputId": "b1a04f91-bd41-4734-c1db-8f294fc9f892"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"qna_chitchat_the_professional.tsv\",sep='\\t')\n",
        "#df.drop(['Source', 'Metadata'])\n",
        "#df1 = df[['user1','user2']]\n",
        "#df1"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-030369996f92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"qna_chitchat_the_professional.tsv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#df.drop(['Source', 'Metadata'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#df1 = df[['user1','user2']]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#df1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'qna_chitchat_the_professional.tsv' does not exist: b'qna_chitchat_the_professional.tsv'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "eE3maeyg60cW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df1 = df[['Question','Answer']]\n",
        "df1\n",
        "df1.to_csv('chat.tsv', sep = '\\t', header=False, index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8mF-5H_v7ZCc",
        "colab_type": "code",
        "outputId": "072b5e8a-8cca-4d06-f28d-d3688effcae7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "cell_type": "code",
      "source": [
        "!head chat.tsv"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "What's your age?\tAge doesn't really apply to me.\n",
            "Are you young?\tAge doesn't really apply to me.\n",
            "When were you born?\tAge doesn't really apply to me.\n",
            "What age are you?\tAge doesn't really apply to me.\n",
            "Are you old?\tAge doesn't really apply to me.\n",
            "How old are you?\tAge doesn't really apply to me.\n",
            "How long ago were you born?\tAge doesn't really apply to me.\n",
            "Ask me anything\tI'm better at answering questions.\n",
            "Ask me a question\tI'm better at answering questions.\n",
            "Can you ask me a question?\tI'm better at answering questions.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LqeAmIF5758A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cat chat.tsv | awk 'NR%10==1' > chat.eng_cmn.test.tsv\n",
        "!cat chat.tsv | awk 'NR%10==2' > chat.eng_cmn.dev.tsv\n",
        "!cat chat.tsv | awk 'NR%10!=1&&NR%10!=2' > chat.eng_cmn.train.tsv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Dr9fgbX08FNO",
        "colab_type": "code",
        "outputId": "a0553d57-fdfa-4ef5-cbf6-e1ba346b4bd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99
        }
      },
      "cell_type": "code",
      "source": [
        "!ls\n",
        "#!head chat.eng_cmn.dev.tsv"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "chat.eng_cmn.dev.tsv\tchat.tsv\n",
            "chat.eng_cmn.test.tsv\tcreate_bitext.py\n",
            "chat.eng_cmn.train.tsv\tqna_chitchat_the_professional.tsv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FlehUdQWKA_p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Build the model"
      ]
    },
    {
      "metadata": {
        "id": "8N69DhrYBlTv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Seq2Seq with attention (non BERT) - seems to be working better than when using BERT"
      ]
    },
    {
      "metadata": {
        "id": "oBotb4-Nq_Xm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!wget -O config.json \n",
        "#!ls\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xPkE0vUqkh_i",
        "colab_type": "code",
        "outputId": "9ba4a0c5-3edf-4188-9008-dfb1aaa8be5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Y47Z5W2drv5y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#this is the shareable link of the file temps.csv in my google drive\n",
        "link = 'https://drive.google.com/open?id=1MkC2EEMqihJPMuw4V4VahLbQO4bdi_B4'\n",
        "fluff, id = link.split('=')\n",
        "#print (id) # Verify that you have everything after '='\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('config.json')  #name of file is irrelevant"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "th4Zb-1NsAar",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!head config.json\n",
        "!mv config.json /content/data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rQs_DmQMBoyi",
        "colab_type": "code",
        "outputId": "a965ab45-5b30-4c9a-f1d7-7e415c9281f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        }
      },
      "cell_type": "code",
      "source": [
        "EN_EMBEDDING_DIM = 256\n",
        "ZH_EMBEDDING_DIM = 256\n",
        "HIDDEN_DIM = 256\n",
        "CUDA_DEVICE = 0\n",
        "\n",
        "serialization_dir = '/content/model/'\n",
        "\n",
        "\n",
        "reader = Seq2SeqDatasetReader(\n",
        "        source_tokenizer=WordTokenizer(),\n",
        "        target_tokenizer=CharacterTokenizer(),\n",
        "        source_token_indexers={'tokens': SingleIdTokenIndexer()},\n",
        "        target_token_indexers={'tokens': SingleIdTokenIndexer(namespace='target_tokens')})\n",
        "train_dataset = reader.read('/content/data/chat.eng_cmn.train.tsv')\n",
        "validation_dataset = reader.read('/content/data/chat.eng_cmn.dev.tsv')\n",
        "\n",
        "vocab = Vocabulary.from_instances(train_dataset + validation_dataset,\n",
        "                                  min_count={'tokens': 3, 'target_tokens': 3})\n",
        "\n",
        "en_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
        "                         embedding_dim=EN_EMBEDDING_DIM)\n",
        "# encoder = PytorchSeq2SeqWrapper(\n",
        "#     torch.nn.LSTM(EN_EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))\n",
        "encoder = StackedSelfAttentionEncoder(input_dim=EN_EMBEDDING_DIM, hidden_dim=HIDDEN_DIM, projection_dim=128, feedforward_hidden_dim=128, num_layers=1, num_attention_heads=8)\n",
        "\n",
        "source_embedder = BasicTextFieldEmbedder({\"tokens\": en_embedding})\n",
        "\n",
        "\n",
        "\n",
        "# attention = LinearAttention(HIDDEN_DIM, HIDDEN_DIM, activation=Activation.by_name('tanh')())\n",
        "# attention = BilinearAttention(HIDDEN_DIM, HIDDEN_DIM)\n",
        "attention = DotProductAttention()\n",
        "\n",
        "max_decoding_steps = 100   # TODO: make this variable\n",
        "model = SimpleSeq2Seq(vocab, source_embedder, encoder, max_decoding_steps,\n",
        "                      target_embedding_dim=ZH_EMBEDDING_DIM,\n",
        "                      target_namespace='target_tokens',\n",
        "                      attention=attention,\n",
        "                      beam_size=8,\n",
        "                      use_bleu=True)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "iterator = BucketIterator(batch_size=32, sorting_keys=[(\"source_tokens\", \"num_tokens\")])\n",
        "\n",
        "iterator.index_with(vocab)\n",
        "\n",
        "CUDA_DEVICE = 0\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    cuda_device = 0\n",
        "    model = model.cuda(cuda_device)\n",
        "else:\n",
        "    cuda_device = -1\n",
        "    \n",
        "\n",
        "trainer = Trainer(model=model,\n",
        "                      optimizer=optimizer,\n",
        "                      iterator=iterator,\n",
        "                      train_dataset=train_dataset,\n",
        "                      validation_dataset=validation_dataset,\n",
        "                      num_epochs=1,\n",
        "                      cuda_device=CUDA_DEVICE)\n",
        "                      #serialization_dir=serialization_dir,                 \n",
        "                          #num_serialized_models_to_keep=2,\n",
        "                          #keep_serialized_model_every_num_seconds=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "525it [00:00, 10903.74it/s]\u001b[A\u001b[A\n",
            "\n",
            "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "66it [00:00, 6379.61it/s]\u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/591 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 591/591 [00:00<00:00, 27521.80it/s]\u001b[A\u001b[AYou provided a validation dataset but patience was set to None, meaning that early stopping is disabled\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "NLSQpjXLuhdZ",
        "colab_type": "code",
        "outputId": "cb279d80-3611-44c6-f83c-f20814fbabe8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "vocab.get_vocab_size('tokens')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "159"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        }
      ]
    },
    {
      "metadata": {
        "id": "582F51ZqB2jh",
        "colab_type": "code",
        "outputId": "8a1c13cb-a2d1-4b7f-c7d1-178e14e21c59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1425
        }
      },
      "cell_type": "code",
      "source": [
        "for i in range(4):\n",
        "    print('Epoch: {}'.format(i))\n",
        "    metrics = trainer.train()\n",
        "\n",
        "    predictor = SimpleSeq2SeqPredictor(model, reader)\n",
        "\n",
        "    #do predictions on last epoch\n",
        "    if i >= 49:\n",
        "      for instance in itertools.islice(validation_dataset, 10):\n",
        "          print('SOURCE:', instance.fields['source_tokens'].tokens)\n",
        "          print('GOLD:', instance.fields['target_tokens'].tokens)\n",
        "          print('PRED:', predictor.predict_instance(instance)['predicted_tokens'])\n",
        "          \n",
        "metrics"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  0%|          | 0/17 [00:00<?, ?it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "loss: 3.9738 ||:   6%|▌         | 1/17 [00:00<00:04,  3.53it/s]\u001b[A\n",
            "loss: 3.7976 ||:  18%|█▊        | 3/17 [00:00<00:03,  4.64it/s]\u001b[A\n",
            "loss: 3.6487 ||:  29%|██▉       | 5/17 [00:00<00:02,  5.89it/s]\u001b[A\n",
            "loss: 3.5319 ||:  41%|████      | 7/17 [00:00<00:01,  7.09it/s]\u001b[A\n",
            "loss: 3.4468 ||:  53%|█████▎    | 9/17 [00:00<00:00,  8.27it/s]\u001b[A\n",
            "loss: 3.4014 ||:  65%|██████▍   | 11/17 [00:00<00:00,  9.86it/s]\u001b[A\n",
            "loss: 3.3575 ||:  76%|███████▋  | 13/17 [00:01<00:00, 10.92it/s]\u001b[A\n",
            "loss: 3.3196 ||:  88%|████████▊ | 15/17 [00:01<00:00, 12.19it/s]\u001b[A\n",
            "loss: 3.2835 ||: 100%|██████████| 17/17 [00:01<00:00, 13.16it/s]\u001b[A\n",
            "\u001b[A\n",
            "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "BLEU: 0.0000, loss: 3.0496 ||:  33%|███▎      | 1/3 [00:00<00:00,  3.37it/s]\u001b[A\n",
            "BLEU: 0.0000, loss: 3.0700 ||:  67%|██████▋   | 2/3 [00:00<00:00,  3.30it/s]\u001b[A\n",
            "BLEU: 0.0000, loss: 3.0542 ||: 100%|██████████| 3/3 [00:00<00:00,  4.03it/s]\u001b[A\n",
            "\u001b[A\n",
            "  0%|          | 0/17 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "loss: 2.9837 ||:  12%|█▏        | 2/17 [00:00<00:01, 14.55it/s]\u001b[A\n",
            "loss: 3.0088 ||:  24%|██▎       | 4/17 [00:00<00:00, 14.74it/s]\u001b[A\n",
            "loss: 3.0331 ||:  35%|███▌      | 6/17 [00:00<00:00, 14.58it/s]\u001b[A\n",
            "loss: 3.0362 ||:  47%|████▋     | 8/17 [00:00<00:00, 14.66it/s]\u001b[A\n",
            "loss: 3.0389 ||:  59%|█████▉    | 10/17 [00:00<00:00, 15.26it/s]\u001b[A\n",
            "loss: 3.0336 ||:  71%|███████   | 12/17 [00:00<00:00, 15.63it/s]\u001b[A\n",
            "loss: 3.0310 ||:  82%|████████▏ | 14/17 [00:00<00:00, 15.68it/s]\u001b[A\n",
            "loss: 3.0249 ||:  94%|█████████▍| 16/17 [00:01<00:00, 15.77it/s]\u001b[A\n",
            "loss: 3.0215 ||: 100%|██████████| 17/17 [00:01<00:00, 15.44it/s]\u001b[A\n",
            "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "BLEU: 0.0000, loss: 2.9610 ||:  33%|███▎      | 1/3 [00:00<00:00,  3.05it/s]\u001b[A\n",
            "BLEU: 0.0000, loss: 2.9760 ||:  67%|██████▋   | 2/3 [00:00<00:00,  2.98it/s]\u001b[A\n",
            "BLEU: 0.0000, loss: 2.9837 ||: 100%|██████████| 3/3 [00:00<00:00,  3.71it/s]\u001b[A\n",
            "\u001b[A\n",
            "  0%|          | 0/17 [00:00<?, ?it/s]\u001b[A\n",
            "loss: 2.9953 ||:  12%|█▏        | 2/17 [00:00<00:01, 14.69it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "loss: 2.9730 ||:  24%|██▎       | 4/17 [00:00<00:00, 15.25it/s]\u001b[A\n",
            "loss: 2.9579 ||:  35%|███▌      | 6/17 [00:00<00:00, 14.78it/s]\u001b[A\n",
            "loss: 2.9500 ||:  47%|████▋     | 8/17 [00:00<00:00, 14.56it/s]\u001b[A\n",
            "loss: 2.9509 ||:  59%|█████▉    | 10/17 [00:00<00:00, 15.06it/s]\u001b[A\n",
            "loss: 2.9511 ||:  71%|███████   | 12/17 [00:00<00:00, 14.88it/s]\u001b[A\n",
            "loss: 2.9448 ||:  82%|████████▏ | 14/17 [00:00<00:00, 15.16it/s]\u001b[A\n",
            "loss: 2.9342 ||:  94%|█████████▍| 16/17 [00:01<00:00, 15.57it/s]\u001b[A\n",
            "loss: 2.9296 ||: 100%|██████████| 17/17 [00:01<00:00, 15.07it/s]\u001b[A\n",
            "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "BLEU: 0.0000, loss: 2.8552 ||:  33%|███▎      | 1/3 [00:00<00:00,  3.26it/s]\u001b[A\n",
            "BLEU: 0.0000, loss: 2.8664 ||:  67%|██████▋   | 2/3 [00:00<00:00,  3.23it/s]\u001b[A\n",
            "BLEU: 0.0000, loss: 2.8909 ||: 100%|██████████| 3/3 [00:00<00:00,  3.92it/s]\u001b[A\n",
            "\u001b[A\n",
            "  0%|          | 0/17 [00:00<?, ?it/s]\u001b[A\n",
            "loss: 2.8491 ||:  12%|█▏        | 2/17 [00:00<00:00, 15.77it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "loss: 2.8516 ||:  24%|██▎       | 4/17 [00:00<00:00, 15.67it/s]\u001b[A\n",
            "loss: 2.8474 ||:  35%|███▌      | 6/17 [00:00<00:00, 16.54it/s]\u001b[A\n",
            "loss: 2.8226 ||:  47%|████▋     | 8/17 [00:00<00:00, 15.83it/s]\u001b[A\n",
            "loss: 2.8155 ||:  59%|█████▉    | 10/17 [00:00<00:00, 16.38it/s]\u001b[A\n",
            "loss: 2.8017 ||:  71%|███████   | 12/17 [00:00<00:00, 15.66it/s]\u001b[A\n",
            "loss: 2.7854 ||:  82%|████████▏ | 14/17 [00:00<00:00, 15.31it/s]\u001b[A\n",
            "loss: 2.7845 ||:  94%|█████████▍| 16/17 [00:01<00:00, 15.02it/s]\u001b[A\n",
            "loss: 2.7789 ||: 100%|██████████| 17/17 [00:01<00:00, 15.44it/s]\u001b[A\n",
            "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "BLEU: 0.0000, loss: 2.6512 ||:  33%|███▎      | 1/3 [00:00<00:00,  3.12it/s]\u001b[A\n",
            "BLEU: 0.0000, loss: 2.6688 ||:  67%|██████▋   | 2/3 [00:00<00:00,  3.13it/s]\u001b[A\n",
            "BLEU: 0.0062, loss: 2.7077 ||: 100%|██████████| 3/3 [00:00<00:00,  3.83it/s]\u001b[A\n",
            "\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'best_epoch': 0,\n",
              " 'best_validation_BLEU': 0.0062080515814421575,\n",
              " 'best_validation_loss': 2.707716464996338,\n",
              " 'epoch': 0,\n",
              " 'peak_cpu_memory_MB': 3264.216,\n",
              " 'peak_gpu_0_memory_MB': 825,\n",
              " 'training_cpu_memory_MB': 3264.216,\n",
              " 'training_duration': '00:00:01',\n",
              " 'training_epochs': 0,\n",
              " 'training_gpu_0_memory_MB': 825,\n",
              " 'training_loss': 2.778914044885074,\n",
              " 'training_start_epoch': 0,\n",
              " 'validation_BLEU': 0.0062080515814421575,\n",
              " 'validation_loss': 2.707716464996338}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "metadata": {
        "id": "fgyAbqkhBx61",
        "colab_type": "code",
        "outputId": "7f950d51-7bb3-4aa3-988e-983a0ec300ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2782
        }
      },
      "cell_type": "code",
      "source": [
        "!allennlp train -r -s /content/model4 /content/data/config.json \n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-04-28 11:10:06,771 - INFO - pytorch_pretrained_bert.modeling - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
            "2019-04-28 11:10:07,817 - INFO - allennlp.common.params - random_seed = 13370\n",
            "2019-04-28 11:10:07,818 - INFO - allennlp.common.params - numpy_seed = 1337\n",
            "2019-04-28 11:10:07,818 - INFO - allennlp.common.params - pytorch_seed = 133\n",
            "2019-04-28 11:10:07,824 - INFO - allennlp.common.checks - Pytorch version: 1.0.1.post2\n",
            "2019-04-28 11:10:07,825 - INFO - allennlp.training.util - Recovering from prior training at /content/model4.\n",
            "2019-04-28 11:10:07,839 - INFO - allennlp.common.params - evaluate_on_test = False\n",
            "2019-04-28 11:10:07,839 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {'lazy': True, 'source_token_indexers': {'tokens': {'namespace': 'source_tokens', 'type': 'single_id'}}, 'source_tokenizer': {'type': 'word'}, 'target_token_indexers': {'tokens': {'namespace': 'target_tokens', 'type': 'single_id'}}, 'target_tokenizer': {'type': 'character'}, 'type': 'seq2seq'} and extras set()\n",
            "2019-04-28 11:10:07,840 - INFO - allennlp.common.params - dataset_reader.type = seq2seq\n",
            "2019-04-28 11:10:07,840 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.dataset_readers.seq2seq.Seq2SeqDatasetReader'> from params {'lazy': True, 'source_token_indexers': {'tokens': {'namespace': 'source_tokens', 'type': 'single_id'}}, 'source_tokenizer': {'type': 'word'}, 'target_token_indexers': {'tokens': {'namespace': 'target_tokens', 'type': 'single_id'}}, 'target_tokenizer': {'type': 'character'}} and extras set()\n",
            "2019-04-28 11:10:07,840 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.tokenizers.tokenizer.Tokenizer'> from params {'type': 'word'} and extras set()\n",
            "2019-04-28 11:10:07,840 - INFO - allennlp.common.params - dataset_reader.source_tokenizer.type = word\n",
            "2019-04-28 11:10:07,840 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.tokenizers.word_tokenizer.WordTokenizer'> from params {} and extras set()\n",
            "2019-04-28 11:10:07,840 - INFO - allennlp.common.params - dataset_reader.source_tokenizer.start_tokens = None\n",
            "2019-04-28 11:10:07,840 - INFO - allennlp.common.params - dataset_reader.source_tokenizer.end_tokens = None\n",
            "2019-04-28 11:10:08,147 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.tokenizers.tokenizer.Tokenizer'> from params {'type': 'character'} and extras set()\n",
            "2019-04-28 11:10:08,147 - INFO - allennlp.common.params - dataset_reader.target_tokenizer.type = character\n",
            "2019-04-28 11:10:08,147 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.tokenizers.character_tokenizer.CharacterTokenizer'> from params {} and extras set()\n",
            "2019-04-28 11:10:08,147 - INFO - allennlp.common.params - dataset_reader.target_tokenizer.byte_encoding = None\n",
            "2019-04-28 11:10:08,148 - INFO - allennlp.common.params - dataset_reader.target_tokenizer.lowercase_characters = False\n",
            "2019-04-28 11:10:08,148 - INFO - allennlp.common.params - dataset_reader.target_tokenizer.start_tokens = None\n",
            "2019-04-28 11:10:08,148 - INFO - allennlp.common.params - dataset_reader.target_tokenizer.end_tokens = None\n",
            "2019-04-28 11:10:08,148 - INFO - allennlp.common.from_params - instantiating class allennlp.data.token_indexers.token_indexer.TokenIndexer from params {'namespace': 'source_tokens', 'type': 'single_id'} and extras set()\n",
            "2019-04-28 11:10:08,148 - INFO - allennlp.common.params - dataset_reader.source_token_indexers.tokens.type = single_id\n",
            "2019-04-28 11:10:08,148 - INFO - allennlp.common.from_params - instantiating class allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer from params {'namespace': 'source_tokens'} and extras set()\n",
            "2019-04-28 11:10:08,148 - INFO - allennlp.common.params - dataset_reader.source_token_indexers.tokens.namespace = source_tokens\n",
            "2019-04-28 11:10:08,148 - INFO - allennlp.common.params - dataset_reader.source_token_indexers.tokens.lowercase_tokens = False\n",
            "2019-04-28 11:10:08,148 - INFO - allennlp.common.params - dataset_reader.source_token_indexers.tokens.start_tokens = None\n",
            "2019-04-28 11:10:08,149 - INFO - allennlp.common.params - dataset_reader.source_token_indexers.tokens.end_tokens = None\n",
            "2019-04-28 11:10:08,149 - INFO - allennlp.common.from_params - instantiating class allennlp.data.token_indexers.token_indexer.TokenIndexer from params {'namespace': 'target_tokens', 'type': 'single_id'} and extras set()\n",
            "2019-04-28 11:10:08,149 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.type = single_id\n",
            "2019-04-28 11:10:08,149 - INFO - allennlp.common.from_params - instantiating class allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer from params {'namespace': 'target_tokens'} and extras set()\n",
            "2019-04-28 11:10:08,149 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.namespace = target_tokens\n",
            "2019-04-28 11:10:08,149 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.lowercase_tokens = False\n",
            "2019-04-28 11:10:08,149 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.start_tokens = None\n",
            "2019-04-28 11:10:08,149 - INFO - allennlp.common.params - dataset_reader.target_token_indexers.tokens.end_tokens = None\n",
            "2019-04-28 11:10:08,149 - INFO - allennlp.common.params - dataset_reader.source_add_start_token = True\n",
            "2019-04-28 11:10:08,149 - INFO - allennlp.common.params - dataset_reader.delimiter = \t\n",
            "2019-04-28 11:10:08,150 - INFO - allennlp.common.params - dataset_reader.lazy = True\n",
            "2019-04-28 11:10:08,150 - INFO - allennlp.common.params - validation_dataset_reader = None\n",
            "2019-04-28 11:10:08,150 - INFO - allennlp.common.params - train_data_path = /content/data/chat.eng_cmn.train.tsv\n",
            "2019-04-28 11:10:08,150 - INFO - allennlp.training.util - Reading training data from /content/data/chat.eng_cmn.train.tsv\n",
            "2019-04-28 11:10:08,150 - INFO - allennlp.common.params - validation_data_path = /content/data/chat.eng_cmn.dev.tsv\n",
            "2019-04-28 11:10:08,150 - INFO - allennlp.training.util - Reading validation data from /content/data/chat.eng_cmn.dev.tsv\n",
            "2019-04-28 11:10:08,150 - INFO - allennlp.common.params - test_data_path = None\n",
            "2019-04-28 11:10:08,150 - INFO - allennlp.training.trainer - From dataset instances, train, validation will be considered for vocabulary creation.\n",
            "2019-04-28 11:10:08,150 - INFO - allennlp.data.vocabulary - Loading token dictionary from /content/model4/vocabulary.\n",
            "2019-04-28 11:10:08,151 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.models.model.Model'> from params {'attention': {'type': 'dot_product'}, 'beam_size': 8, 'encoder': {'hidden_size': 256, 'input_size': 256, 'num_layers': 2, 'type': 'lstm'}, 'max_decoding_steps': 100, 'source_embedder': {'tokens': {'embedding_dim': 256, 'trainable': True, 'type': 'embedding', 'vocab_namespace': 'source_tokens'}, 'type': 'basic'}, 'target_embedding_dim': 256, 'target_namespace': 'target_tokens', 'type': 'simple_seq2seq', 'use_bleu': True} and extras {'vocab'}\n",
            "2019-04-28 11:10:08,151 - INFO - allennlp.common.params - model.type = simple_seq2seq\n",
            "2019-04-28 11:10:08,151 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.models.encoder_decoders.simple_seq2seq.SimpleSeq2Seq'> from params {'attention': {'type': 'dot_product'}, 'beam_size': 8, 'encoder': {'hidden_size': 256, 'input_size': 256, 'num_layers': 2, 'type': 'lstm'}, 'max_decoding_steps': 100, 'source_embedder': {'tokens': {'embedding_dim': 256, 'trainable': True, 'type': 'embedding', 'vocab_namespace': 'source_tokens'}, 'type': 'basic'}, 'target_embedding_dim': 256, 'target_namespace': 'target_tokens', 'use_bleu': True} and extras {'vocab'}\n",
            "2019-04-28 11:10:08,151 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder'> from params {'tokens': {'embedding_dim': 256, 'trainable': True, 'type': 'embedding', 'vocab_namespace': 'source_tokens'}, 'type': 'basic'} and extras {'vocab'}\n",
            "2019-04-28 11:10:08,151 - INFO - allennlp.common.params - model.source_embedder.type = basic\n",
            "2019-04-28 11:10:08,152 - INFO - allennlp.common.params - model.source_embedder.embedder_to_indexer_map = None\n",
            "2019-04-28 11:10:08,152 - INFO - allennlp.common.params - model.source_embedder.allow_unmatched_keys = False\n",
            "2019-04-28 11:10:08,152 - INFO - allennlp.common.params - model.source_embedder.token_embedders = None\n",
            "2019-04-28 11:10:08,152 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'embedding_dim': 256, 'trainable': True, 'type': 'embedding', 'vocab_namespace': 'source_tokens'} and extras {'vocab'}\n",
            "2019-04-28 11:10:08,152 - INFO - allennlp.common.params - model.source_embedder.tokens.type = embedding\n",
            "2019-04-28 11:10:08,152 - INFO - allennlp.common.params - model.source_embedder.tokens.num_embeddings = None\n",
            "2019-04-28 11:10:08,152 - INFO - allennlp.common.params - model.source_embedder.tokens.vocab_namespace = source_tokens\n",
            "2019-04-28 11:10:08,152 - INFO - allennlp.common.params - model.source_embedder.tokens.embedding_dim = 256\n",
            "2019-04-28 11:10:08,152 - INFO - allennlp.common.params - model.source_embedder.tokens.pretrained_file = None\n",
            "2019-04-28 11:10:08,152 - INFO - allennlp.common.params - model.source_embedder.tokens.projection_dim = None\n",
            "2019-04-28 11:10:08,152 - INFO - allennlp.common.params - model.source_embedder.tokens.trainable = True\n",
            "2019-04-28 11:10:08,153 - INFO - allennlp.common.params - model.source_embedder.tokens.padding_index = None\n",
            "2019-04-28 11:10:08,153 - INFO - allennlp.common.params - model.source_embedder.tokens.max_norm = None\n",
            "2019-04-28 11:10:08,153 - INFO - allennlp.common.params - model.source_embedder.tokens.norm_type = 2.0\n",
            "2019-04-28 11:10:08,153 - INFO - allennlp.common.params - model.source_embedder.tokens.scale_grad_by_freq = False\n",
            "2019-04-28 11:10:08,153 - INFO - allennlp.common.params - model.source_embedder.tokens.sparse = False\n",
            "2019-04-28 11:10:08,154 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder'> from params {'hidden_size': 256, 'input_size': 256, 'num_layers': 2, 'type': 'lstm'} and extras {'vocab'}\n",
            "2019-04-28 11:10:08,154 - INFO - allennlp.common.params - model.encoder.type = lstm\n",
            "2019-04-28 11:10:08,154 - INFO - allennlp.common.params - model.encoder.batch_first = True\n",
            "2019-04-28 11:10:08,154 - INFO - allennlp.common.params - model.encoder.stateful = False\n",
            "2019-04-28 11:10:08,154 - INFO - allennlp.common.params - Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
            "2019-04-28 11:10:08,154 - INFO - allennlp.common.params - CURRENTLY DEFINED PARAMETERS: \n",
            "2019-04-28 11:10:08,154 - INFO - allennlp.common.params - model.encoder.hidden_size = 256\n",
            "2019-04-28 11:10:08,154 - INFO - allennlp.common.params - model.encoder.input_size = 256\n",
            "2019-04-28 11:10:08,154 - INFO - allennlp.common.params - model.encoder.num_layers = 2\n",
            "2019-04-28 11:10:08,154 - INFO - allennlp.common.params - model.encoder.batch_first = True\n",
            "2019-04-28 11:10:08,163 - INFO - allennlp.common.params - model.max_decoding_steps = 100\n",
            "2019-04-28 11:10:08,163 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.attention.attention.Attention'> from params {'type': 'dot_product'} and extras {'vocab'}\n",
            "2019-04-28 11:10:08,163 - INFO - allennlp.common.params - model.attention.type = dot_product\n",
            "2019-04-28 11:10:08,163 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.attention.dot_product_attention.DotProductAttention'> from params {} and extras {'vocab'}\n",
            "2019-04-28 11:10:08,163 - INFO - allennlp.common.params - model.attention.normalize = True\n",
            "2019-04-28 11:10:08,164 - INFO - allennlp.common.params - model.beam_size = 8\n",
            "2019-04-28 11:10:08,164 - INFO - allennlp.common.params - model.target_namespace = target_tokens\n",
            "2019-04-28 11:10:08,164 - INFO - allennlp.common.params - model.target_embedding_dim = 256\n",
            "2019-04-28 11:10:08,164 - INFO - allennlp.common.params - model.scheduled_sampling_ratio = 0.0\n",
            "2019-04-28 11:10:08,164 - INFO - allennlp.common.params - model.use_bleu = True\n",
            "2019-04-28 11:10:08,171 - INFO - root - Loading a model trained before embedding extension was implemented; pass an explicit vocab namespace if you want to extend the vocabulary.\n",
            "2019-04-28 11:10:08,172 - WARNING - root - vocabulary serialization directory /content/model4/vocabulary is not empty\n",
            "2019-04-28 11:10:08,173 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.iterators.data_iterator.DataIterator'> from params {'batch_size': 32, 'padding_noise': 0, 'sorting_keys': [['source_tokens', 'num_tokens']], 'type': 'bucket'} and extras set()\n",
            "2019-04-28 11:10:08,173 - INFO - allennlp.common.params - iterator.type = bucket\n",
            "2019-04-28 11:10:08,173 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.iterators.bucket_iterator.BucketIterator'> from params {'batch_size': 32, 'padding_noise': 0, 'sorting_keys': [['source_tokens', 'num_tokens']]} and extras set()\n",
            "2019-04-28 11:10:08,173 - INFO - allennlp.common.params - iterator.sorting_keys = [['source_tokens', 'num_tokens']]\n",
            "2019-04-28 11:10:08,173 - INFO - allennlp.common.params - iterator.padding_noise = 0\n",
            "2019-04-28 11:10:08,173 - INFO - allennlp.common.params - iterator.biggest_batch_first = False\n",
            "2019-04-28 11:10:08,173 - INFO - allennlp.common.params - iterator.batch_size = 32\n",
            "2019-04-28 11:10:08,173 - INFO - allennlp.common.params - iterator.instances_per_epoch = None\n",
            "2019-04-28 11:10:08,173 - INFO - allennlp.common.params - iterator.max_instances_in_memory = None\n",
            "2019-04-28 11:10:08,174 - INFO - allennlp.common.params - iterator.cache_instances = False\n",
            "2019-04-28 11:10:08,174 - INFO - allennlp.common.params - iterator.track_epoch = False\n",
            "2019-04-28 11:10:08,174 - INFO - allennlp.common.params - iterator.maximum_samples_per_batch = None\n",
            "2019-04-28 11:10:08,174 - INFO - allennlp.common.params - validation_iterator = None\n",
            "2019-04-28 11:10:08,174 - INFO - allennlp.common.params - trainer.no_grad = ()\n",
            "2019-04-28 11:10:08,174 - INFO - allennlp.training.trainer - Following parameters are Frozen  (without gradient):\n",
            "2019-04-28 11:10:08,174 - INFO - allennlp.training.trainer - Following parameters are Tunable (with gradient):\n",
            "2019-04-28 11:10:08,174 - INFO - allennlp.training.trainer - _source_embedder.token_embedder_tokens.weight\n",
            "2019-04-28 11:10:08,174 - INFO - allennlp.training.trainer - _encoder._module.weight_ih_l0\n",
            "2019-04-28 11:10:08,174 - INFO - allennlp.training.trainer - _encoder._module.weight_hh_l0\n",
            "2019-04-28 11:10:08,174 - INFO - allennlp.training.trainer - _encoder._module.bias_ih_l0\n",
            "2019-04-28 11:10:08,174 - INFO - allennlp.training.trainer - _encoder._module.bias_hh_l0\n",
            "2019-04-28 11:10:08,174 - INFO - allennlp.training.trainer - _encoder._module.weight_ih_l1\n",
            "2019-04-28 11:10:08,174 - INFO - allennlp.training.trainer - _encoder._module.weight_hh_l1\n",
            "2019-04-28 11:10:08,175 - INFO - allennlp.training.trainer - _encoder._module.bias_ih_l1\n",
            "2019-04-28 11:10:08,175 - INFO - allennlp.training.trainer - _encoder._module.bias_hh_l1\n",
            "2019-04-28 11:10:08,175 - INFO - allennlp.training.trainer - _target_embedder.weight\n",
            "2019-04-28 11:10:08,175 - INFO - allennlp.training.trainer - _decoder_cell.weight_ih\n",
            "2019-04-28 11:10:08,175 - INFO - allennlp.training.trainer - _decoder_cell.weight_hh\n",
            "2019-04-28 11:10:08,175 - INFO - allennlp.training.trainer - _decoder_cell.bias_ih\n",
            "2019-04-28 11:10:08,175 - INFO - allennlp.training.trainer - _decoder_cell.bias_hh\n",
            "2019-04-28 11:10:08,175 - INFO - allennlp.training.trainer - _output_projection_layer.weight\n",
            "2019-04-28 11:10:08,175 - INFO - allennlp.training.trainer - _output_projection_layer.bias\n",
            "2019-04-28 11:10:08,175 - INFO - allennlp.common.params - trainer.patience = 50\n",
            "2019-04-28 11:10:08,175 - INFO - allennlp.common.params - trainer.validation_metric = -loss\n",
            "2019-04-28 11:10:08,175 - INFO - allennlp.common.params - trainer.shuffle = True\n",
            "2019-04-28 11:10:08,175 - INFO - allennlp.common.params - trainer.num_epochs = 50\n",
            "2019-04-28 11:10:08,175 - INFO - allennlp.common.params - trainer.cuda_device = 0\n",
            "2019-04-28 11:10:08,175 - INFO - allennlp.common.params - trainer.grad_norm = None\n",
            "2019-04-28 11:10:08,175 - INFO - allennlp.common.params - trainer.grad_clipping = None\n",
            "2019-04-28 11:10:08,175 - INFO - allennlp.common.params - trainer.learning_rate_scheduler = None\n",
            "2019-04-28 11:10:08,175 - INFO - allennlp.common.params - trainer.momentum_scheduler = None\n",
            "2019-04-28 11:10:12,489 - INFO - allennlp.common.params - trainer.optimizer.type = adam\n",
            "2019-04-28 11:10:12,489 - INFO - allennlp.common.params - trainer.optimizer.parameter_groups = None\n",
            "2019-04-28 11:10:12,490 - INFO - allennlp.training.optimizers - Number of trainable parameters: 1868340\n",
            "2019-04-28 11:10:12,491 - INFO - allennlp.common.params - trainer.optimizer.infer_type_and_cast = True\n",
            "2019-04-28 11:10:12,491 - INFO - allennlp.common.params - Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
            "2019-04-28 11:10:12,491 - INFO - allennlp.common.params - CURRENTLY DEFINED PARAMETERS: \n",
            "2019-04-28 11:10:12,491 - INFO - allennlp.common.params - trainer.optimizer.lr = 0.01\n",
            "2019-04-28 11:10:12,491 - INFO - allennlp.common.params - trainer.num_serialized_models_to_keep = 20\n",
            "2019-04-28 11:10:12,491 - INFO - allennlp.common.params - trainer.keep_serialized_model_every_num_seconds = None\n",
            "2019-04-28 11:10:12,491 - INFO - allennlp.common.params - trainer.model_save_interval = None\n",
            "2019-04-28 11:10:12,491 - INFO - allennlp.common.params - trainer.summary_interval = 100\n",
            "2019-04-28 11:10:12,491 - INFO - allennlp.common.params - trainer.histogram_interval = None\n",
            "2019-04-28 11:10:12,491 - INFO - allennlp.common.params - trainer.should_log_parameter_statistics = True\n",
            "2019-04-28 11:10:12,491 - INFO - allennlp.common.params - trainer.should_log_learning_rate = False\n",
            "2019-04-28 11:10:12,492 - INFO - allennlp.common.params - trainer.log_batch_size_period = None\n",
            "2019-04-28 11:10:12,510 - INFO - root - Generating grammar tables from /usr/lib/python3.6/lib2to3/Grammar.txt\n",
            "2019-04-28 11:10:12,536 - INFO - root - Generating grammar tables from /usr/lib/python3.6/lib2to3/PatternGrammar.txt\n",
            "2019-04-28 11:10:12,595 - INFO - allennlp.training.trainer - Beginning training.\n",
            "2019-04-28 11:10:12,595 - INFO - allennlp.training.checkpointer - loading best weights\n",
            "2019-04-28 11:10:12,604 - INFO - allennlp.models.archival - archiving weights and vocabulary to /content/model4/model.tar.gz\n",
            "2019-04-28 11:10:13,057 - INFO - allennlp.common.util - Metrics: {\n",
            "  \"best_epoch\": 49,\n",
            "  \"best_validation_BLEU\": 0.028111481023588795,\n",
            "  \"best_validation_loss\": 0.22773526112238565\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FM9AksUgeOiB",
        "colab_type": "code",
        "outputId": "61dcaaf3-3690-4b9e-8243-1eda385a5a6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from allennlp.models.archival import load_archive\n",
        "from allennlp.service.predictors import Predictor\n",
        "\n",
        "path = '/content/model4/model.tar.gz'\n",
        "archive = load_archive(path)\n",
        "\n",
        "#archive = load_archive(serialization_dir+'model.tar.gz')\n",
        "predictor = Predictor.from_archive(archive, 'simple_seq2seq') \n",
        "''.join(predictor.predict(\"What are you doing tomorrow?\")['predicted_tokens'])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I apologize.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "T0S_4iyLDjnw",
        "colab_type": "code",
        "outputId": "9ee35295-36c7-4cde-910f-9e138d0fa44b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        }
      },
      "cell_type": "code",
      "source": [
        "for instance in itertools.islice(train_dataset, 10):\n",
        "    print('SOURCE:', instance.fields['source_tokens'].tokens)\n",
        "    print('GOLD:', instance.fields['target_tokens'].tokens)\n",
        "    print('PRED:', ''.join(predictor.predict_instance(instance)['predicted_tokens']))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-ababb054097f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0minstance\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mislice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SOURCE:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'source_tokens'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GOLD:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target_tokens'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'PRED:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predicted_tokens'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "srkKPJ_UE8O4",
        "colab_type": "code",
        "outputId": "dc6b0eb1-d25e-48c1-a91f-7513cce874c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "''.join(predictor.predict_instance(instance)['predicted_tokens'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ok.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "PPKEB_kiVLE7",
        "colab_type": "code",
        "outputId": "a4591564-a419-45d5-8811-12a4fc25ff2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 14255
        }
      },
      "cell_type": "code",
      "source": [
        "predictor.predict(\"whats your name?\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'class_log_probabilities': [-4.412934303283691,\n",
              "  -13.126253128051758,\n",
              "  -18.1739444732666,\n",
              "  -22.792346954345703,\n",
              "  -23.302249908447266,\n",
              "  -27.92108154296875,\n",
              "  -29.777690887451172,\n",
              "  -125.58854675292969],\n",
              " 'predicted_tokens': ['O', 'k', '.'],\n",
              " 'predictions': [[38,\n",
              "   29,\n",
              "   9,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11],\n",
              "  [17,\n",
              "   2,\n",
              "   19,\n",
              "   5,\n",
              "   7,\n",
              "   18,\n",
              "   3,\n",
              "   2,\n",
              "   3,\n",
              "   13,\n",
              "   6,\n",
              "   3,\n",
              "   9,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11],\n",
              "  [17,\n",
              "   2,\n",
              "   19,\n",
              "   5,\n",
              "   7,\n",
              "   18,\n",
              "   3,\n",
              "   2,\n",
              "   3,\n",
              "   13,\n",
              "   6,\n",
              "   3,\n",
              "   2,\n",
              "   3,\n",
              "   13,\n",
              "   6,\n",
              "   3,\n",
              "   9,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11],\n",
              "  [17,\n",
              "   2,\n",
              "   19,\n",
              "   5,\n",
              "   7,\n",
              "   18,\n",
              "   3,\n",
              "   2,\n",
              "   3,\n",
              "   13,\n",
              "   6,\n",
              "   3,\n",
              "   2,\n",
              "   3,\n",
              "   13,\n",
              "   6,\n",
              "   3,\n",
              "   2,\n",
              "   13,\n",
              "   6,\n",
              "   3,\n",
              "   9,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11],\n",
              "  [17,\n",
              "   2,\n",
              "   19,\n",
              "   5,\n",
              "   7,\n",
              "   18,\n",
              "   3,\n",
              "   2,\n",
              "   3,\n",
              "   13,\n",
              "   6,\n",
              "   3,\n",
              "   2,\n",
              "   3,\n",
              "   13,\n",
              "   6,\n",
              "   3,\n",
              "   2,\n",
              "   3,\n",
              "   13,\n",
              "   6,\n",
              "   3,\n",
              "   9,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11],\n",
              "  [17,\n",
              "   2,\n",
              "   19,\n",
              "   5,\n",
              "   7,\n",
              "   18,\n",
              "   3,\n",
              "   2,\n",
              "   3,\n",
              "   13,\n",
              "   6,\n",
              "   3,\n",
              "   2,\n",
              "   3,\n",
              "   13,\n",
              "   6,\n",
              "   3,\n",
              "   2,\n",
              "   3,\n",
              "   13,\n",
              "   6,\n",
              "   3,\n",
              "   2,\n",
              "   13,\n",
              "   6,\n",
              "   3,\n",
              "   9,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11],\n",
              "  [17,\n",
              "   2,\n",
              "   19,\n",
              "   5,\n",
              "   7,\n",
              "   18,\n",
              "   3,\n",
              "   2,\n",
              "   3,\n",
              "   13,\n",
              "   6,\n",
              "   3,\n",
              "   2,\n",
              "   3,\n",
              "   13,\n",
              "   6,\n",
              "   3,\n",
              "   2,\n",
              "   3,\n",
              "   13,\n",
              "   6,\n",
              "   25,\n",
              "   4,\n",
              "   2,\n",
              "   3,\n",
              "   13,\n",
              "   6,\n",
              "   3,\n",
              "   9,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11,\n",
              "   11],\n",
              "  [17,\n",
              "   2,\n",
              "   19,\n",
              "   5,\n",
              "   7,\n",
              "   18,\n",
              "   3,\n",
              "   2,\n",
              "   3,\n",
              "   13,\n",
              "   6,\n",
              "   3,\n",
              "   2,\n",
              "   3,\n",
              "   13,\n",
              "   6,\n",
              "   3,\n",
              "   2,\n",
              "   3,\n",
              "   13,\n",
              "   6,\n",
              "   25,\n",
              "   4,\n",
              "   2,\n",
              "   3,\n",
              "   13,\n",
              "   6,\n",
              "   3,\n",
              "   2,\n",
              "   6,\n",
              "   3,\n",
              "   2,\n",
              "   6,\n",
              "   3,\n",
              "   2,\n",
              "   6,\n",
              "   3,\n",
              "   2,\n",
              "   6,\n",
              "   3,\n",
              "   2,\n",
              "   6,\n",
              "   3,\n",
              "   2,\n",
              "   6,\n",
              "   3,\n",
              "   2,\n",
              "   6,\n",
              "   3,\n",
              "   2,\n",
              "   6,\n",
              "   3,\n",
              "   2,\n",
              "   6,\n",
              "   3,\n",
              "   2,\n",
              "   6,\n",
              "   3,\n",
              "   2,\n",
              "   6,\n",
              "   3,\n",
              "   2,\n",
              "   6,\n",
              "   3,\n",
              "   2,\n",
              "   6,\n",
              "   3,\n",
              "   2,\n",
              "   6,\n",
              "   3,\n",
              "   2,\n",
              "   6,\n",
              "   3,\n",
              "   2,\n",
              "   6,\n",
              "   3,\n",
              "   2,\n",
              "   6,\n",
              "   3,\n",
              "   2,\n",
              "   6,\n",
              "   3,\n",
              "   2,\n",
              "   6,\n",
              "   3,\n",
              "   2,\n",
              "   6,\n",
              "   3,\n",
              "   2,\n",
              "   6,\n",
              "   3,\n",
              "   2,\n",
              "   6,\n",
              "   3,\n",
              "   2,\n",
              "   6,\n",
              "   3,\n",
              "   2,\n",
              "   6,\n",
              "   3]]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "metadata": {
        "id": "9GGa076oZgkO",
        "colab_type": "code",
        "outputId": "d7744bbb-055c-47cb-fd3f-59747e2352de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "cell_type": "code",
      "source": [
        "!ls -l"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 16512\n",
            "-rw-r--r-- 1 root root 4222127 Apr 28 09:49 best.th\n",
            "-rw-r--r-- 1 root root    1708 Apr 28 09:53 config.json\n",
            "drwxr-xr-x 4 root root    4096 Apr 28 09:45 log\n",
            "-rw-r--r-- 1 root root     480 Apr 28 09:49 metrics_epoch_0.json\n",
            "-rw-r--r-- 1 root root 4222127 Apr 28 09:49 model_state_epoch_0.th\n",
            "-rw-r--r-- 1 root root      30 Apr 28 09:50 model.tar.gz\n",
            "-rw-r--r-- 1 root root 8441816 Apr 28 09:49 training_state_epoch_0.th\n",
            "drwxr-xr-x 2 root root    4096 Apr 28 09:45 vocabulary\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "34Y6_yP6aIX1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "vocab.save_to_files(os.path.join(serialization_dir, \"vocabulary\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WdrgYyRIaOWI",
        "colab_type": "code",
        "outputId": "fb279ee7-29be-48fe-e154-fea6bc41f706",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "%cd vocabulary\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/model/vocabulary\n",
            "non_padded_namespaces.txt  target_tokens.txt  tokens.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "83rTzDM7adf_",
        "colab_type": "code",
        "outputId": "f164deb0-f4b0-4734-cfe6-fb8e8f828d91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "%cd model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/model\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}